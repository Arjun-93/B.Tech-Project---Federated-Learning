# -*- coding: utf-8 -*-
"""Client_Server.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J48zh5Ln-udMaGRrLtXrcN-m7pOcaMCe

# Clients Side Training
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn

df = pd.read_csv("/content/feature_selected_voice_data.csv")

idx = int(len(df)*0.5)
client1_dataset = df[:idx]
client2_dataset = df[idx:]

# Client1 dataset -->
client1_X = client1_dataset.iloc[:,:-1]
client1_Y = client1_dataset["label"]
le = preprocessing.LabelEncoder()
client1_Y = le.fit_transform(client1_Y)
client1_X = client1_X.to_numpy()
# client1_Y = client1_Y.to_numpy()

X_train_1 = client1_X.astype('float32')
y_train_1 = client1_Y.astype('float32')

X_train_1 = torch.from_numpy(X_train_1)
y_train_1 =torch.from_numpy(y_train_1)

# Client2 Datatset -->
client2_X = client2_dataset.iloc[:,:-1]
client2_Y = client2_dataset["label"]
le = preprocessing.LabelEncoder()
client2_Y = le.fit_transform(client2_Y)
client2_X = client2_X.to_numpy()
# client2_Y = client2_Y.to_numpy()

X_train_2 = client2_X.astype('float32')
y_train_2 = client2_Y.astype('float32')

X_train_2 = torch.from_numpy(X_train_2)
y_train_2 =torch.from_numpy(y_train_2)

n_samples, n_features = X_train_1.shape

class LogisticRegression(nn.Module):
    def __init__(self, n_input_features):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(n_input_features, 1)
    
    #sigmoid transformation of the input 
    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred

def clients_training(X_train, y_train, lr):
  num_epochs = 500
  learning_rate = 0.0001 
  criterion = nn.BCELoss() # Binary cross Entropy loss                              
  optimizer = torch.optim.SGD(lr.parameters(), lr=learning_rate) 
  error_loss = []
  for epoch in range(num_epochs):
      train_loss = 0
      optimizer.zero_grad()
      y_pred = lr(X_train)
      loss = criterion(y_pred.reshape(1584), y_train)             
      loss.backward()
      optimizer.step()
      # if (epoch+1) % 20 == 0:                                          
      #     print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')
      train_loss += loss.item()*X_train.size(0)
      train_loss = train_loss/1584
      error_loss.append(train_loss)
  total_loss = sum(error_loss)/len(error_loss)
  return lr.state_dict(), total_loss

#print(lr.parameters()) # printing learnable parameters (weights and biases)
#print(lr.state_dict) # pint dictionary of all parameters
# print(total_loss) # printing total loss in training the model

# weights, loss = clients_training(X_train_1, y_train_1)

"""# Server Side Training"""

import time
import copy
from copy import deepcopy
import matplotlib.pyplot as plt

"""Train """

def ss_train_r(model, rounds, plt_title="Loss Curve"):

  
  # global model weights
  global_weights = model.state_dict()

  # training loss
  train_loss = []

  # measure time
  start = time.time()


  for current_round in range(1, rounds+1):

    w, local_loss = [], []

    # model1 = LogisticRegression(n_features)
    weights1, loss1 = clients_training(X_train_1, y_train_1, model)

    # model2 = LogisticRegression(n_features)
    weights2, loss2 = clients_training(X_train_2, y_train_2, model)

    w.append(copy.deepcopy(weights1))
    w.append(copy.deepcopy(weights2))
    # print(loss1)
    # print(loss2)
    local_loss.append(copy.deepcopy(loss1))
    local_loss.append(copy.deepcopy(loss2))

    # updating the global weights
    weights_avg = copy.deepcopy(w[0])
    for k in weights_avg.keys():
      for i in range(1, len(w)):
        weights_avg[k] += w[i][k]

      weights_avg[k] = torch.div(weights_avg[k], len(w))
    global_weights = weights_avg

    # move the updated weights to our model state dict
    model.load_state_dict(global_weights)

    # loss
    loss_avg = sum(local_loss) / len(local_loss)
    print('Round: {}... \tAverage Loss: {}'.format(current_round, round(loss_avg, 3)))
    train_loss.append(loss_avg)

  plt.plot(train_loss)
  end = time.time()
  print("Training Done!")
  print("Total time taken to Train: {}".format(end-start))

  return model

model = LogisticRegression(n_features)

if torch.cuda.is_available():
  model.cuda()

server_training = ss_train_r(model, 100,"Loss Curve")

"""# Training and Testing"""

def data_partition(dataset, clients):
  num_items_per_client = int(len(dataset)/clients)
  client_dict = {}
  image_idxs = [i for i in range(len(dataset))]

  for i in range(clients):
    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))
    image_idxs = list(set(image_idxs) - client_dict[i])

  return client_dict

import pandas as pd
import numpy as np

data = pd.read_csv("/content/feature_selected_voice_data.csv")

temp = data_partition(data, 2)

for key,value in temp.items():
	print(key, ':', value)

from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms, utils, datasets
from torchsummary import summary

class CustomDataset(Dataset):
  def __init__(self, dataset, idxs):
      self.dataset = dataset
      self.idxs = list(idxs)

  def __len__(self):
      return len(self.idxs)

  def __getitem__(self, item):
      X_train, y_train = self.dataset[self.idxs[item]]
      return X_train, y_train

train_loader = DataLoader(CustomDataset(data, temp[1]), batch_size=100, shuffle=True)



for X_train, y_train in train_loader:
  if torch.cuda.is_available():
    X_train, y_train = X_train.cuda(), y_train.cuda()

# logistic regression class
class LogisticRegression(nn.Module):
    def __init__(self, n_input_features):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(n_input_features, 1)
    
    #sigmoid transformation of the input 
    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred

Model_LR = LogisticRegression()

import torch
import torch.nn as nn
import torch.nn.functional as F

def train(model):
  for idxs in range(2):
    train_loader = DataLoader(CustomDataset(data, temp[0]), batch_size=100, shuffle=True)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)
    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)

    
    for X_train, y_train in train_loader:
      if torch.cuda.is_available():
        X_train, y_train = X_train.cuda(), y_train.cuda()